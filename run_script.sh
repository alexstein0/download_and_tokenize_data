python download_and_tokenize_data.py data=long_context tokenizer='google/gemma-2-2b' sequence_length=2048
#python download_and_tokenize_data.py data=long_context tokenizer='google/gemma-2-2b' sequence_length=4096
#python download_and_tokenize_data.py data=long_context tokenizer='google/gemma-2-2b' sequence_length=8192
#python download_and_tokenize_data.py data=long_context tokenizer='google/gemma-2-2b' sequence_length=16384
#python download_and_tokenize_data.py data=long_context tokenizer='google/gemma-2-2b' sequence_length=32768
# python preprocess_data.py data=GSM8K_C4 data.data_sampling=even data.processed_extension=alex
# python preprocess_data.py data=GSM8K_C4 data.processed_extension=alex
# python preprocess_data.py data=default data.processed_extension=alex
# python preprocess_data.py data=default data.sources=[C4] data.processed_extension=alex

#python download_and_tokenize_data.py data=long_context tokenizer='meta-llama/Llama-2-7b-chat-hf' sequence_length=2048
#python download_and_tokenize_data.py data=long_context tokenizer='meta-llama/Llama-2-7b-chat-hf' sequence_length=4096
#python download_and_tokenize_data.py data=long_context tokenizer='meta-llama/Llama-2-7b-chat-hf' sequence_length=8192
#python download_and_tokenize_data.py data=long_context tokenizer='meta-llama/Llama-2-7b-chat-hf' sequence_length=16384
#python download_and_tokenize_data.py data=long_context tokenizer='meta-llama/Llama-2-7b-chat-hf' sequence_length=32768
